{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Sg4JPAtn7vYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy scipy torchvision"
      ],
      "metadata": {
        "id": "mqPoJHsk-fC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\""
      ],
      "metadata": {
        "id": "ZhKTvuSr-joa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6b4e141"
      },
      "source": [
        "# Install a specific version of numpy to avoid conflicts\n",
        "%pip install numpy==1.26.4 --force-reinstall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Consolidated Pipeline - GenConViT Ensemble Model (AI-Only Scoring)\n",
        "# This script uses a dual autoencoder (AE and VAE) model for deepfake\n",
        "# detection. The final verdict is based exclusively on the AI model's\n",
        "# prediction, with forensic analysis provided for context only.\n",
        "\n",
        "# --- 1. SETUP AND INSTALLATIONS ---\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "from collections import OrderedDict\n",
        "\n",
        "def install_packages(packages):\n",
        "    \"\"\"Installs required Python packages using pip.\"\"\"\n",
        "    print(\"Upgrading pip...\")\n",
        "    try:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--upgrade\", \"pip\"], check=True)\n",
        "        print(f\"Installing/Updating packages: {', '.join(packages)}\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--no-cache-dir\"] + packages, check=True)\n",
        "        importlib.invalidate_caches()\n",
        "        print(\"All required packages are installed.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"ERROR: Failed to install packages. {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "print(\"Checking for required packages...\")\n",
        "# A comprehensive list of dependencies for all forensic analyses.\n",
        "install_packages([\n",
        "    \"numpy<2.0\", \"torch\", \"torchvision\", \"pandas\", \"opencv-python\",\n",
        "    \"Pillow\", \"moviepy\", \"mediapipe\", \"scikit-image\", \"scipy\",\n",
        "    \"exifread\", \"librosa\", \"dlib\", \"face_alignment\"\n",
        "])\n",
        "\n",
        "# --- Imports (post-install) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import exifread\n",
        "import librosa\n",
        "import dlib\n",
        "import face_alignment\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageChops\n",
        "from moviepy.editor import VideoFileClip\n",
        "from scipy.signal import welch\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# Ensure the device is set correctly for PyTorch operations.\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "# --- 2. GENCONVIT MODEL ARCHITECTURE ---\n",
        "# Defines the neural network components for the GenConViT autoencoders.\n",
        "\n",
        "class ED_Encoder(nn.Module):\n",
        "    \"\"\"The convolutional encoder part of the GenConViT model.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "class ED_Decoder_Config1(nn.Module):\n",
        "    \"\"\"Decoder for the AE model, reconstructs to full resolution (224x224).\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 16, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 3, 2, 2), nn.ReLU(True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "class ED_Decoder_Config2(nn.Module):\n",
        "    \"\"\"Decoder for the VAE model, reconstructs to half resolution (112x112).\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 64, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 16, 2, 2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 3, 2, 2), nn.ReLU(True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "class GenConViT(nn.Module):\n",
        "    \"\"\"The complete GenConViT model, combining an encoder and a decoder.\"\"\"\n",
        "    def __init__(self, decoder_config=1):\n",
        "        super().__init__()\n",
        "        self.encoder = ED_Encoder()\n",
        "        self.decoder = ED_Decoder_Config1() if decoder_config == 1 else ED_Decoder_Config2()\n",
        "\n",
        "    def forward(self, images):\n",
        "        encimg = self.encoder(images)\n",
        "        decimg = self.decoder(encimg)\n",
        "        return decimg\n",
        "\n",
        "\n",
        "# --- 3. CONFIGURE DIRECTORIES & LOAD ASSETS ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Drive mounted.\")\n",
        "except ImportError:\n",
        "    print(\"Not in a Colab environment. Using local directories.\")\n",
        "    pass\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/AuraVerity-Reports' if os.path.exists('/content/drive/MyDrive') else os.path.expanduser('~/AuraVerity-Reports')\n",
        "REPORTS_DIR = os.path.join(BASE_DIR, 'reports')\n",
        "EXPLAIN_DIR = os.path.join(BASE_DIR, 'explainability_outputs')\n",
        "TEMP_DIR = os.path.join(BASE_DIR, 'temp')\n",
        "for path in [REPORTS_DIR, EXPLAIN_DIR, TEMP_DIR]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "print(f\"\\nProject directories configured in: {BASE_DIR}\")\n",
        "\n",
        "DLIB_MODEL_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
        "if not os.path.exists(DLIB_MODEL_PATH):\n",
        "    print(\"Downloading dlib facial landmark predictor...\")\n",
        "    os.system(\"wget -q http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    os.system(\"bzip2 -df shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    print(\"Dlib model downloaded.\")\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "landmark_predictor = dlib.shape_predictor(DLIB_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- 4. AI MODEL LOADING & PREDICTION ---\n",
        "def load_genconvit_models(ed_path, vae_path):\n",
        "    \"\"\"Loads both the AE and VAE GenConViT models from specified paths.\"\"\"\n",
        "    loaded_models = {}\n",
        "\n",
        "    def _load_single_model(weight_path, decoder_config):\n",
        "        if not os.path.exists(weight_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {weight_path}\")\n",
        "        model = GenConViT(decoder_config=decoder_config)\n",
        "        checkpoint = torch.load(weight_path, map_location=DEVICE)\n",
        "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
        "\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k.replace('module.', '').replace('model.', '')\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict, strict=False)\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    print(\"\\nLoading GenConViT-ED model...\")\n",
        "    loaded_models['ed'] = _load_single_model(ed_path, decoder_config=1)\n",
        "    print(\"Loading GenConViT-VAE model...\")\n",
        "    loaded_models['vae'] = _load_single_model(vae_path, decoder_config=2)\n",
        "\n",
        "    return loaded_models\n",
        "\n",
        "# --- Model Paths (USER-PROVIDED) ---\n",
        "ED_MODEL_PATH = '/content/drive/MyDrive/ThruthChain-v2-Models/weights/genconvit_ed_inference.pth'\n",
        "VAE_MODEL_PATH = '/content/drive/MyDrive/ThruthChain-v2-Models/weights/genconvit_vae_inference.pth'\n",
        "\n",
        "try:\n",
        "    loaded_models = load_genconvit_models(ED_MODEL_PATH, VAE_MODEL_PATH)\n",
        "    print(\"Successfully loaded GenConViT ensemble models.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Could not load models. {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "def predict_with_genconvit_ensemble(image_path, models):\n",
        "    \"\"\"\n",
        "    Generates a fake probability score based on the average reconstruction\n",
        "    error from the AE and VAE models.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            recon_ed = models['ed'](img_tensor)\n",
        "            recon_vae_half = models['vae'](img_tensor)\n",
        "            recon_vae = F.resize(recon_vae_half, img_tensor.shape[-2:],\n",
        "                                 interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "\n",
        "            mse_loss = nn.MSELoss()\n",
        "            error_ed = mse_loss(recon_ed, img_tensor).item()\n",
        "            error_vae = mse_loss(recon_vae, img_tensor).item()\n",
        "            avg_mse = (error_ed + error_vae) / 2.0\n",
        "\n",
        "            k = 0.5\n",
        "            fake_prob = 1 - np.exp(-k * avg_mse)\n",
        "\n",
        "            return {\n",
        "                \"fake_probability\": float(fake_prob),\n",
        "                \"avg_mse\": float(avg_mse),\n",
        "                \"original_tensor\": img_tensor,\n",
        "                \"recon_ed\": recon_ed,\n",
        "                \"recon_vae\": recon_vae\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction error: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_error_map(pred_data, output_path):\n",
        "    \"\"\"\n",
        "    Generates and saves a visual heatmap of the reconstruction error.\n",
        "    \"\"\"\n",
        "    print(\"-> Generating reconstruction error map...\")\n",
        "    try:\n",
        "        original = pred_data['original_tensor'].squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "        recon_ed = pred_data['recon_ed'].squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "        recon_vae = pred_data['recon_vae'].squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "        mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
        "        original, recon_ed, recon_vae = [np.clip(std * img + mean, 0, 1) for img in [original, recon_ed, recon_vae]]\n",
        "\n",
        "        avg_error = (np.abs(original - recon_ed) + np.abs(original - recon_vae)) / 2.0\n",
        "\n",
        "        error_gray = np.mean(avg_error, axis=2)\n",
        "        error_map = (error_gray / np.max(error_gray) * 255).astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(error_map, cv2.COLORMAP_HOT)\n",
        "\n",
        "        original_uint8 = (original * 255).astype(np.uint8)\n",
        "        superimposed = cv2.addWeighted(heatmap, 0.6, cv2.cvtColor(original_uint8, cv2.COLOR_RGB2BGR), 0.4, 0)\n",
        "\n",
        "        cv2.imwrite(output_path, superimposed)\n",
        "        return {\"status\": \"performed\", \"output_path\": output_path}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "\n",
        "# --- 5. FORENSIC ANALYSIS FUNCTIONS ---\n",
        "# (These functions are independent of the core AI model and remain unchanged)\n",
        "\n",
        "def analyze_prnu(image_path):\n",
        "    print(\"-> Running PRNU Analysis...\")\n",
        "    try:\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None: return {\"status\": \"error\", \"message\": \"Cannot read image\"}\n",
        "        denoised = cv2.medianBlur(img, 3)\n",
        "        residual = img.astype(float) - denoised.astype(float)\n",
        "        return {\"status\": \"performed\", \"residual_variance\": float(np.var(residual))}\n",
        "    except Exception as e: return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "def analyze_eye_reflections(image_path):\n",
        "    print(\"-> Running Eye Reflection Consistency Analysis...\")\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray)\n",
        "        if not faces: return {\"status\": \"performed\", \"face_detected\": False}\n",
        "        landmarks = landmark_predictor(gray, faces[0])\n",
        "        left_pts = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])\n",
        "        right_pts = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])\n",
        "        (lx, ly, lw, lh), (rx, ry, rw, rh) = cv2.boundingRect(left_pts), cv2.boundingRect(right_pts)\n",
        "        if min(lw, lh, rw, rh) < 6: return {\"status\": \"performed\", \"face_detected\": True, \"roi_quality\": \"low\"}\n",
        "        left_eye, right_eye = gray[ly:ly+lh, lx:lx+lw], gray[ry:ry+rh, rx:rx+rw]\n",
        "        left_hist = cv2.calcHist([left_eye], [0], None, [256], [0, 256])\n",
        "        right_hist = cv2.calcHist([right_eye], [0], None, [256], [0, 256])\n",
        "        corr = cv2.compareHist(left_hist, right_hist, cv2.HISTCMP_CORREL)\n",
        "        return {\"status\": \"performed\", \"face_detected\": True, \"reflection_correlation\": round(float(corr), 4)}\n",
        "    except Exception as e: return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "def analyze_pulse(video_path):\n",
        "    print(\"-> Running Physiological Signal (Pulse) Analysis...\")\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        means, fps = [], cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "        max_frames = int(min(fps * 8, cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "        count = 0\n",
        "        while count < max_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            faces = face_detector(gray)\n",
        "            if faces:\n",
        "                lm = landmark_predictor(gray, faces[0])\n",
        "                pts = np.array([(lm.part(i).x, lm.part(i).y) for i in [19, 24, 27]])\n",
        "                (x, y, w, h) = cv2.boundingRect(pts)\n",
        "                roi = frame[y:y+h, x:x+w, 1]\n",
        "                if roi.size > 0: means.append(np.mean(roi))\n",
        "            count += 1\n",
        "        cap.release()\n",
        "        if len(means) < fps: return {\"status\": \"not_enough_data\"}\n",
        "        means_array = np.array(means) - np.mean(means)\n",
        "        freqs, psd = welch(means_array, fs=fps, nperseg=min(len(means_array), 256))\n",
        "        bpm = freqs[np.argmax(psd)] * 60\n",
        "        is_plausible = 40 <= bpm <= 180\n",
        "        return {\"status\": \"performed\", \"estimated_bpm\": round(bpm, 2), \"is_plausible\": is_plausible}\n",
        "    except Exception as e: return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "def analyze_lip_sync(video_path):\n",
        "    print(\"-> Running Audio-Visual (Lip-Sync) Analysis...\")\n",
        "    try:\n",
        "        with VideoFileClip(video_path) as clip:\n",
        "            if not clip.audio: return {\"status\": \"no_audio\"}\n",
        "            audio_path = os.path.join(TEMP_DIR, \"temp_lipsync_audio.wav\")\n",
        "            clip.audio.write_audiofile(audio_path, codec='pcm_s16le', logger=None)\n",
        "        y, sr = librosa.load(audio_path, sr=None, mono=True)\n",
        "        os.remove(audio_path)\n",
        "        audio_energy = librosa.feature.rms(y=y)[0]\n",
        "        fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=str(DEVICE), flip_input=False)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        mouth_openings = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            preds = fa.get_landmarks(frame)\n",
        "            if preds:\n",
        "                mouth_pts = preds[0][48:68]\n",
        "                mouth_height = euclidean(mouth_pts[13], mouth_pts[19])\n",
        "                mouth_openings.append(mouth_height)\n",
        "            else:\n",
        "                mouth_openings.append(0)\n",
        "        cap.release()\n",
        "        if len(mouth_openings) < 5 or len(audio_energy) < 5: return {\"status\": \"not_enough_data\"}\n",
        "        min_len = min(len(audio_energy), len(mouth_openings))\n",
        "        a = (audio_energy[:min_len] - np.mean(audio_energy[:min_len])) / (np.std(audio_energy[:min_len]) + 1e-8)\n",
        "        m = (np.array(mouth_openings[:min_len]) - np.mean(mouth_openings[:min_len])) / (np.std(mouth_openings[:min_len]) + 1e-8)\n",
        "        corr = np.corrcoef(a, m)[0, 1]\n",
        "        return {\"status\": \"performed\", \"correlation_score\": round(float(corr), 4)}\n",
        "    except Exception as e: return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "def analyze_ela(image_path, quality=90):\n",
        "    print(\"-> Running Error Level Analysis (ELA)...\")\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        buffer = BytesIO()\n",
        "        img.save(buffer, format='JPEG', quality=quality)\n",
        "        resaved = Image.open(buffer)\n",
        "        ela = ImageChops.difference(img, resaved)\n",
        "        extrema = ela.getextrema()\n",
        "        max_diff = max([ex[1] for ex in extrema])\n",
        "        return {\"status\": \"performed\", \"max_ela_difference\": max_diff}\n",
        "    except Exception as e: return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "def analyze_metadata(media_path):\n",
        "    print(\"-> Running Metadata analysis...\")\n",
        "    try:\n",
        "        with open(media_path, 'rb') as f:\n",
        "            tags = exifread.process_file(f, details=False)\n",
        "        software = tags.get('Image Software')\n",
        "        if software and any(ed in str(software).lower() for ed in (\"photoshop\", \"gimp\", \"stable diffusion\")):\n",
        "            return {\"status\": \"performed\", \"anomalies_detected\": True, \"software\": str(software)}\n",
        "        return {\"status\": \"performed\", \"anomalies_detected\": False}\n",
        "    except Exception: return {\"status\": \"no_exif_data\"}\n",
        "\n",
        "def analyze_audio_spectrogram(audio_path):\n",
        "    print(\"-> Running Audio Spectrogram Analysis...\")\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        spec = np.abs(librosa.stft(y))\n",
        "        freq_cutoff_strength = np.mean(spec[-int(spec.shape[0]*0.1):, :]) / (np.mean(spec) + 1e-8)\n",
        "        is_suspicious = freq_cutoff_strength < 0.1\n",
        "        return {\"status\": \"performed\", \"freq_cutoff_strength\": round(float(freq_cutoff_strength), 4), \"is_suspicious\": is_suspicious}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "\n",
        "# --- 6. HOLISTIC SCORING & REPORTING ---\n",
        "def calculate_holistic_score(ai_fake_prob, fr):\n",
        "    \"\"\"\n",
        "    Passes through the AI model's score and lists forensic findings as evidence\n",
        "    without altering the final prediction.\n",
        "    \"\"\"\n",
        "    # The final score is now determined SOLELY by the AI model's prediction.\n",
        "    fake_score = 0.5 if ai_fake_prob is None else np.clip(ai_fake_prob, 0.0, 1.0)\n",
        "    truth_score = 1.0 - fake_score\n",
        "\n",
        "    # The evidence list will now just report the findings without score changes.\n",
        "    evidence = [f\"AI Model Fake Probability: {fake_score:.2%}\"]\n",
        "\n",
        "    if fr.get('prnu_analysis', {}).get('residual_variance', 0) > 2.0:\n",
        "        evidence.append(\"[Forensic Finding] High PRNU residual variance detected.\")\n",
        "    if fr.get('eye_reflections', {}).get('reflection_correlation', 1.0) < 0.7:\n",
        "        evidence.append(\"[Forensic Finding] Inconsistent eye reflections detected.\")\n",
        "    if not fr.get('pulse_analysis', {}).get('is_plausible', True):\n",
        "        evidence.append(\"[Forensic Finding] Anomalous physiological pulse detected.\")\n",
        "    if fr.get('metadata', {}).get('anomalies_detected', False):\n",
        "        evidence.append(\"[Forensic Finding] Editing software detected in metadata.\")\n",
        "    if fr.get('lip_sync_analysis', {}).get('correlation_score', 1.0) < 0.2:\n",
        "        evidence.append(\"[Forensic Finding] Poor lip-sync correlation detected.\")\n",
        "    if fr.get('audio_spectrogram', {}).get('is_suspicious', False):\n",
        "        evidence.append(\"[Forensic Finding] Suspicious audio spectrogram characteristics detected.\")\n",
        "\n",
        "    return {\"truth_score\": truth_score, \"fake_score\": fake_score, \"evidence\": evidence}\n",
        "\n",
        "def build_final_report(media_path, holistic, ai_results, forensics, visuals, inconclusive_reason=None):\n",
        "    \"\"\"Constructs the final JSON report.\"\"\"\n",
        "    class NpEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            if isinstance(obj, (np.integer, np.floating, np.bool_)): return obj.item()\n",
        "            return super().default(obj)\n",
        "\n",
        "    report = {\n",
        "        \"report_timestamp\": datetime.now().isoformat(),\n",
        "        \"media_analyzed\": os.path.basename(media_path),\n",
        "        \"holistic_truth_score\": holistic[\"truth_score\"],\n",
        "        \"holistic_fake_score\": holistic[\"fake_score\"],\n",
        "        \"final_verdict\": \"INCONCLUSIVE\" if inconclusive_reason else (\"REAL\" if holistic[\"truth_score\"] >= 0.5 else \"FAKE\"),\n",
        "        \"inconclusive_reason\": inconclusive_reason,\n",
        "        \"evidence_summary\": holistic[\"evidence\"],\n",
        "        \"ai_model_prediction\": ai_results,\n",
        "        \"detailed_forensic_findings\": forensics,\n",
        "        \"visual_explainability\": visuals\n",
        "    }\n",
        "    return json.loads(json.dumps(report, cls=NpEncoder, indent=2))\n",
        "\n",
        "\n",
        "# --- 7. MAIN PIPELINE ORCHESTRATION ---\n",
        "def run_ultimate_pipeline(media_path, models):\n",
        "    \"\"\"The main function that orchestrates the entire analysis process.\"\"\"\n",
        "    print(f\"\\n{'='*30}\\nRunning Pipeline for: {os.path.basename(media_path)}\\n{'='*30}\")\n",
        "\n",
        "    if not os.path.exists(media_path):\n",
        "        print(\"ERROR: File not found.\")\n",
        "        return\n",
        "    if os.path.getsize(media_path) > 50 * 1024 * 1024:\n",
        "        print(\"ERROR: File exceeds 50MB size limit.\")\n",
        "        return\n",
        "\n",
        "    ext = os.path.splitext(media_path)[1].lower()\n",
        "    img_fmts, vid_fmts, aud_fmts = ['.jpg', '.jpeg', '.png', '.webp', '.bmp'], ['.mp4', '.mov', '.avi', '.webm', '.mkv'], ['.wav', '.mp3', '.flac', '.ogg']\n",
        "    media_type = 'image' if ext in img_fmts else 'video' if ext in vid_fmts else 'audio' if ext in aud_fmts else 'unsupported'\n",
        "\n",
        "    if media_type == 'unsupported':\n",
        "        print(f\"ERROR: Unsupported file format '{ext}'.\")\n",
        "        return\n",
        "\n",
        "    ai_results, forensics, visuals, inconclusive_reason = {}, {}, {}, None\n",
        "\n",
        "    if media_type in ['image', 'video']:\n",
        "        frame_path = media_path\n",
        "        if media_type == 'video':\n",
        "            cap = cv2.VideoCapture(media_path)\n",
        "            ret, frame = cap.read()\n",
        "            cap.release()\n",
        "            if not ret:\n",
        "                print(\"ERROR: Could not read first frame of video.\")\n",
        "                return\n",
        "            frame_path = os.path.join(TEMP_DIR, \"temp_frame.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "\n",
        "        pred_data = predict_with_genconvit_ensemble(frame_path, models)\n",
        "        if pred_data:\n",
        "            ai_results = {\"raw_fake_probability\": pred_data[\"fake_probability\"], \"avg_mse\": pred_data[\"avg_mse\"]}\n",
        "            err_map_path = os.path.join(EXPLAIN_DIR, f\"{os.path.splitext(os.path.basename(media_path))[0]}_error_map.jpg\")\n",
        "            visuals['reconstruction_error_map'] = generate_error_map(pred_data, err_map_path)\n",
        "\n",
        "        forensics['metadata'] = analyze_metadata(frame_path)\n",
        "        forensics['prnu_analysis'] = analyze_prnu(frame_path)\n",
        "        forensics['eye_reflections'] = analyze_eye_reflections(frame_path)\n",
        "        forensics['ela'] = analyze_ela(frame_path)\n",
        "\n",
        "        if media_type == 'video':\n",
        "            forensics['pulse_analysis'] = analyze_pulse(media_path)\n",
        "            forensics['lip_sync_analysis'] = analyze_lip_sync(media_path)\n",
        "            try:\n",
        "                with VideoFileClip(media_path) as clip:\n",
        "                    if clip.audio:\n",
        "                        audio_path = os.path.join(TEMP_DIR, \"temp_audio.wav\")\n",
        "                        clip.audio.write_audiofile(audio_path, codec='pcm_s16le', logger=None)\n",
        "                        forensics['audio_spectrogram'] = analyze_audio_spectrogram(audio_path)\n",
        "                        os.remove(audio_path)\n",
        "            except Exception as e: print(f\"Could not process audio from video: {e}\")\n",
        "            if os.path.exists(frame_path): os.remove(frame_path)\n",
        "\n",
        "    elif media_type == 'audio':\n",
        "        inconclusive_reason = \"AI model analysis skipped (audio-only input).\"\n",
        "        forensics['audio_spectrogram'] = analyze_audio_spectrogram(media_path)\n",
        "\n",
        "    ai_prob = ai_results.get(\"raw_fake_probability\") if ai_results else None\n",
        "    holistic = calculate_holistic_score(ai_prob, forensics)\n",
        "    report = build_final_report(media_path, holistic, ai_results, forensics, visuals, inconclusive_reason)\n",
        "\n",
        "    print(\"\\n--- FINAL HOLISTIC REPORT ---\")\n",
        "    print(json.dumps(report, indent=2))\n",
        "\n",
        "    report_path = os.path.join(REPORTS_DIR, f\"{os.path.splitext(os.path.basename(media_path))[0]}_holistic_report.json\")\n",
        "    with open(report_path, 'w') as f: json.dump(report, f, indent=4)\n",
        "    print(f\"\\nReport saved to {report_path}\")\n",
        "\n",
        "\n",
        "image_test_path = '/content/drive/MyDrive/test-image.jpg'\n",
        "video_test_path = '/content/drive/MyDrive/test-video.mp4'\n",
        "audio_test_path = '/content/drive/MyDrive/test-audio.wav'\n",
        "\n",
        "# Create placeholder files if they are missing, to allow the script to run.\n",
        "if not os.path.exists(image_test_path):\n",
        "    print(f\"Creating placeholder image at: {image_test_path}\")\n",
        "    cv2.imwrite(image_test_path, np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
        "if not os.path.exists(video_test_path):\n",
        "    print(f\"SKIPPING video test: File not found at '{video_test_path}'\")\n",
        "if not os.path.exists(audio_test_path):\n",
        "     print(f\"SKIPPING audio test: File not found at '{audio_test_path}'\")\n",
        "\n",
        "for f in [image_test_path, video_test_path, audio_test_path]:\n",
        "    if os.path.exists(f):\n",
        "        run_ultimate_pipeline(f, loaded_models)\n",
        "    else:\n",
        "        print(f'\\nSKIPPING: File not found at \"{f}\"')\n",
        "\n"
      ],
      "metadata": {
        "id": "ET-W4XVUBmF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [

        "# image_test_path = '/content/ChatGPT Image Jul 14, 2025, 03_21_14 PM.png'\n",
        "video_test_path = '/content/id20_0001.mp4'\n",
        "# audio_test_path = '/content/file100.wav_16k.wav_norm.wav_mono.wav_silence.wav_2sec.wav'\n",
        "\n",
        "if not os.path.exists(video_test_path):\n",
        "    print(f\"SKIPPING video test: File not found at '{video_test_path}'\")\n",
        "\n",
        "\n",
        "\n",
        "# Run the pipeline on the specified files\n",
        "for f in [video_test_path]:\n",
        "    if os.path.exists(f):\n",
        "        run_ultimate_pipeline(f, loaded_models)\n",
        "    else:\n",
        "        print(f'\\nSKIPPING: File not found at \"{f}\"')\n",
        "\n"
      ],
      "metadata": {
        "id": "iXtmEqss_lTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
